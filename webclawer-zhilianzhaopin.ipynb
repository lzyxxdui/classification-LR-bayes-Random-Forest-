{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取爬取的城市和爬取的页码数\n",
    "def get_city_page():\n",
    "    city = input('请输入要爬取的城市：')\n",
    "    page = int(input('请输入要爬取的页码数：'))\n",
    "    return city,page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建url_list\n",
    "def create_url_lis(city,page):\n",
    "    # 城市对应编码\n",
    "    city_code_dict = {\n",
    "        '上海':538, '北京':530, '广州':763, '深圳':765, '天津':531, '武汉':736, '西安':854, \n",
    "        '成都':801, '南京':635, '杭州':653, '重庆':551, '厦门':682,'济南':702, '青岛':703, '郑州':719\n",
    "    }\n",
    "    # 定义一个空列表用于存放url\n",
    "    url_lis = []\n",
    "    # 根据城市中文名取出对应的城市编码\n",
    "    city_code = city_code_dict[city]\n",
    "    # 循环遍历页码数，生成url_lis\n",
    "    for p in range(page):\n",
    "        url = 'https://sou.zhaopin.com/?jl={}&kw={}&p={}'.format(city_code,'数据分析',p+1)\n",
    "        url_lis.append(url)\n",
    "    return url_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入要爬取的城市：北京\n",
      "请输入要爬取的页码数：1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://sou.zhaopin.com/?jl=530&kw=数据分析&p=1']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 以下代码为测试用\n",
    "city,page = get_city_page()\n",
    "url_lis = create_url_lis(city,page)\n",
    "url_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据url获取网页源代码\n",
    "def get_html(url):\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 Edg/109.0.1518.70',\n",
    "        'cookie': 'x-zp-client-id=373889a6-aab3-4eb9-813f-5ac0f06913fc; FSSBBIl1UgzbN7NO=5jB1BXq8Cik69q67XTB0xAGr3udQBbEx6eE3qmI_hl1ZPmIyHJrfpGPW1n3e4hkEDBGA3TZ291IGtpbqZ9b6ZFa; _uab_collina=166800275022344594750875; login-type=b; x-zp-device-id=7f1606ab4ce001192771371b516a2ab0; zp_passport_deepknow_sessionId=8f984297s16e6843ac9e58da24af56d4973b; at=2d4cb15f162249b29deb7c065e7cf535; rt=d4862c038e0e48b79c0d5b31c25f80e0; sts_deviceid=185fcbd18271e7-0217e2b4a5b5ca-7d5d5474-1327104-185fcbd18281f5; sts_sg=1; sts_evtseq=1; sts_sid=185fcbd182c23b-0166e48ecf8d2b-7d5d5474-1327104-185fcbd182f2ed; sts_chnlsid=Unknown; zp_src_url=https%3A%2F%2Fpassport.zhaopin.com%2F; ZP_OLD_FLAG=false; Hm_lvt_38ba284938d5eddca645bb5e02a02006=1674982530; acw_tc=276077cb16749836820493615ebe7f52ef6a41480bdc804e0e89c0c4f4f1a8; sensorsdata2015jssdkcross=%7B%22distinct_id%22%3A%221152342112%22%2C%22first_id%22%3A%221845cb61e9e3bf-065609215c5b4d8-7d5d5474-1327104-1845cb61e9fb69%22%2C%22props%22%3A%7B%22%24latest_traffic_source_type%22%3A%22%E7%9B%B4%E6%8E%A5%E6%B5%81%E9%87%8F%22%2C%22%24latest_search_keyword%22%3A%22%E6%9C%AA%E5%8F%96%E5%88%B0%E5%80%BC_%E7%9B%B4%E6%8E%A5%E6%89%93%E5%BC%80%22%2C%22%24latest_referrer%22%3A%22%22%7D%2C%22identities%22%3A%22eyIkaWRlbnRpdHlfY29va2llX2lkIjoiMTg0NWNiNjFlOWUzYmYtMDY1NjA5MjE1YzViNGQ4LTdkNWQ1NDc0LTEzMjcxMDQtMTg0NWNiNjFlOWZiNjkiLCIkaWRlbnRpdHlfbG9naW5faWQiOiIxMTUyMzQyMTEyIn0%3D%22%2C%22history_login_id%22%3A%7B%22name%22%3A%22%24identity_login_id%22%2C%22value%22%3A%221152342112%22%7D%2C%22%24device_id%22%3A%221845cb61e9e3bf-065609215c5b4d8-7d5d5474-1327104-1845cb61e9fb69%22%7D; locationInfo_search={%22code%22:%22710%22%2C%22name%22:%22%E6%B3%B0%E5%AE%89%22%2C%22message%22:%22%E5%8C%B9%E9%85%8D%E5%88%B0%E5%B8%82%E7%BA%A7%E7%BC%96%E7%A0%81%22}; Hm_lpvt_38ba284938d5eddca645bb5e02a02006=1674983685; ssxmod_itna=Yqfx0iit37qxuDl4iqYI7pxUOIDBlAp=77e=Gx0HPreGzDAxn40iDtPoNgObF7BrTfqb7ZGr3NQIj44b=e78YpEdDU4i8DCMx3NrDem=D5xGoDPxDeDAQK0kUxCKD9DWKDFbq03Dc=508DeIFEaxDUlGhK0KDTx017xGODi5w0eYDma5DRpKDea5D9hPDwaHwlxG7U0EkDm4kKDDzpW4=KBgOUS8zN3OpHfOv7UoFhRELElIC9pKDoEPEKR9RnavLv0nDTeraQexxLliPzm0eKCeyj72BpQGKrlGP80iZKWNK1vXxDDcPaoD; ssxmod_itna2=Yqfx0iit37qxuDl4iqYI7pxUOIDBlAp=77e=DnKSiceDsqNQDLlQQ=uth4QIhdGcDYIxxD==; ZL_REPORT_GLOBAL={%22jobs%22:{%22funczoneShare%22:%22dtl_best_for_you%22%2C%22recommandActionidShare%22:%22c5800697-9fa8-4e20-a27f-885df3f1df82-job%22}}; selectCity_search=530; FSSBBIl1UgzbN7NP=53e3yqbHKiAEqqqDmdJJlPGKjLjYkIQKLjNvqDxRAfyeLRGv5kgRYheUa5JgT1v8h4Cm68chJnQG5C1hKoPNMaVD8Oly.5mDhsiKTqB7Y5QwVjfelfF30y.puCzAu9jKSjv1QSJOMeUduVxZuc9ooB6ctq4Dcf.PoV8rodypDJUawHtkENMt869uAvxJZOIlh9MU4r7iYpl1x6cOg_C3cj23kZiMAQc7MdvBfV_nucEVsxAcxUWVxhumwPThQWI4c2z0Vc7Js506C_Jm3hGbx_lkmJspIL2fqn.DhC8SEP4pu0df0DSviKCjTAUu_smJX1lZXQ6RiToz6uti9X5tTq1'\n",
    "    }\n",
    "    html = requests.get(url,headers = headers).text\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      "<head>\n",
      "<meta name=\"renderer\" content=\"webkit\">\n",
      "<meta name=viewport content=\"width=device-width,initial-scale=1,minimum-scale=1,maximum-scale=1,user-scalable=no,viewport-fit=cover\">\n",
      "<meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge,chrome=1\">\n",
      "<title>招聘（求职）尽在智联招聘</title>\n",
      "<meta http-equiv=\"Cache-Control\" content=\"no-transform\" />\n",
      "<meta http-equiv=\"Cache-Control\" content=\"no-siteapp\" />\n",
      "<meta name=\"description\" content=\"智联招聘为您提供招聘信息,工作尽在智联招聘\" />\n",
      "<meta name=\"keywords\" content=\"招聘,招聘信息\" />\n",
      "<link rel=\"shortcut icon\" type=\"image/png\" href=\"//common-bucket.zhaopin.cn/img/zhilia\n"
     ]
    }
   ],
   "source": [
    "# 以下为测试代码\n",
    "url = 'https://sou.zhaopin.com/?jl=531&kw=%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90&p=1'\n",
    "res = get_html(url)\n",
    "print(res[0:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解析网页源代码，提取想要的信息\n",
    "def transform_html(response):\n",
    "    \"\"\"\n",
    "    解析网页源代码，提取想要的信息，并返回信息的dataframe\n",
    "    response：抓取到的网页源代码\n",
    "    \"\"\"\n",
    "    # 职位名称、薪资、地区、工作经验、学历、职位类别、招聘人数、职位描述、岗位职责、职位福利、公司名称、公司类型、公司规模\n",
    "    html = etree.HTML(response)\n",
    "    # 获取工作名称\n",
    "    job = html.xpath('//span[@class=\"iteminfo__line1__jobname__name\"]/@title')\n",
    "    # 获取薪资范围\n",
    "    salary = html.xpath('//p[@class=\"iteminfo__line2__jobdesc__salary\"]/text()')\n",
    "    for i in range(len(salary)):\n",
    "        salary[i] = salary[i].strip('\\n').strip(' ').rstrip('\\n')\n",
    "    # 获取地区、经验、学历信息\n",
    "    location,experience,education = ([] for i in range(3))\n",
    "    require = html.xpath('//ul[@class=\"iteminfo__line2__jobdesc__demand\"]')\n",
    "    for req in require:\n",
    "        try:\n",
    "            loc = req.xpath('.//li[@class=\"iteminfo__line2__jobdesc__demand__item\"]/text()')[0]\n",
    "            location.append(loc)\n",
    "        except:\n",
    "            location.append(np.nan)\n",
    "        try:\n",
    "            exp = req.xpath('.//li[@class=\"iteminfo__line2__jobdesc__demand__item\"]/text()')[1]\n",
    "            experience.append(exp)\n",
    "        except:\n",
    "            experience.append(np.nan)\n",
    "        try:\n",
    "            edu = req.xpath('.//li[@class=\"iteminfo__line2__jobdesc__demand__item\"]/text()')[2]\n",
    "            education.append(edu)\n",
    "        except:\n",
    "            education.append(np.nan)\n",
    "\n",
    "    # 获取职位标签\n",
    "    job_tag = []\n",
    "    job_tag_lis = html.xpath('//div[@class=\"iteminfo__line3__welfare\"]')\n",
    "    for tag in job_tag_lis:\n",
    "        tag_info = tag.xpath('.//div[@class=\"iteminfo__line3__welfare__item\"]/text()')\n",
    "        tag_info = str(tag_info)\n",
    "        job_tag.append(tag_info)\n",
    "\n",
    "    # 获取公司名称\n",
    "    company_name = html.xpath('//span[@class=\"iteminfo__line1__compname__name\"]/text()')\n",
    "\n",
    "    # 获取公司类型、公司规模\n",
    "    company_type = []\n",
    "    company_size = []\n",
    "    company_detail = html.xpath('//div[@class=\"iteminfo__line2__compdesc\"]')\n",
    "    for company in company_detail:\n",
    "        try:\n",
    "            com_type = company.xpath('.//span[@class=\"iteminfo__line2__compdesc__item\"]/text()')[0]\n",
    "            company_type.append(com_type)\n",
    "        except:\n",
    "            company_type.append(np.nan)\n",
    "        try:\n",
    "            com_size = company.xpath('.//span[@class=\"iteminfo__line2__compdesc__item\"]/text()')[1]\n",
    "            company_size.append(com_size)\n",
    "        except:\n",
    "            company_size.append(np.nan)\n",
    "\n",
    "    data_lis = [job,salary,location,experience,education,job_tag,company_name,company_type,company_size]\n",
    "    \n",
    "    # 爬取结果合成一个dataframe\n",
    "    get_data = pd.DataFrame(columns = ['职位名称','薪资范围','地点','工作经验','学历要求','岗位标签','公司名称','公司类型','公司规模'])\n",
    "    for col,data in zip(get_data.columns,data_lis):\n",
    "        get_data[col] = data\n",
    "    # 返回数据的dataframe\n",
    "    return get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>职位名称</th>\n",
       "      <th>薪资范围</th>\n",
       "      <th>地点</th>\n",
       "      <th>工作经验</th>\n",
       "      <th>学历要求</th>\n",
       "      <th>岗位标签</th>\n",
       "      <th>公司名称</th>\n",
       "      <th>公司类型</th>\n",
       "      <th>公司规模</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>数据分析专员</td>\n",
       "      <td>6千-1.1万</td>\n",
       "      <td>天津-西青</td>\n",
       "      <td>3-5年</td>\n",
       "      <td>大专</td>\n",
       "      <td>['运营数据分析']</td>\n",
       "      <td>北京达佳互联信息技术有限公司</td>\n",
       "      <td>上市公司</td>\n",
       "      <td>10000人以上</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>数据分析师</td>\n",
       "      <td>8千-1.1万</td>\n",
       "      <td>天津-河东</td>\n",
       "      <td>不限</td>\n",
       "      <td>本科</td>\n",
       "      <td>['运营数据分析']</td>\n",
       "      <td>石家庄飞谷互联网信息服务有限公司</td>\n",
       "      <td>股份制企业</td>\n",
       "      <td>1000-9999人</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>数据分析岗</td>\n",
       "      <td>6千-9千</td>\n",
       "      <td>天津</td>\n",
       "      <td>3-5年</td>\n",
       "      <td>大专</td>\n",
       "      <td>['Power BI', '销售数据分析']</td>\n",
       "      <td>爱玛科技集团股份有限公司</td>\n",
       "      <td>上市公司</td>\n",
       "      <td>10000人以上</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>数据分析师</td>\n",
       "      <td>6千-1.2万</td>\n",
       "      <td>天津-西青</td>\n",
       "      <td>不限</td>\n",
       "      <td>本科</td>\n",
       "      <td>['数据分析']</td>\n",
       "      <td>天津卓漫科技有限公司</td>\n",
       "      <td>民营</td>\n",
       "      <td>500-999人</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>大数据分析师</td>\n",
       "      <td>8千-1.2万</td>\n",
       "      <td>天津-河西</td>\n",
       "      <td>不限</td>\n",
       "      <td>大专</td>\n",
       "      <td>['R语言', 'SPSS', '数据分析', '大数据分析']</td>\n",
       "      <td>北京中阔鼎盛科技有限公司</td>\n",
       "      <td></td>\n",
       "      <td>20-99人</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     职位名称     薪资范围     地点  工作经验 学历要求                              岗位标签  \\\n",
       "0  数据分析专员  6千-1.1万  天津-西青  3-5年   大专                        ['运营数据分析']   \n",
       "1   数据分析师  8千-1.1万  天津-河东    不限   本科                        ['运营数据分析']   \n",
       "2   数据分析岗    6千-9千     天津  3-5年   大专            ['Power BI', '销售数据分析']   \n",
       "3   数据分析师  6千-1.2万  天津-西青    不限   本科                          ['数据分析']   \n",
       "4  大数据分析师  8千-1.2万  天津-河西    不限   大专  ['R语言', 'SPSS', '数据分析', '大数据分析']   \n",
       "\n",
       "               公司名称    公司类型         公司规模  \n",
       "0    北京达佳互联信息技术有限公司   上市公司     10000人以上   \n",
       "1  石家庄飞谷互联网信息服务有限公司  股份制企业   1000-9999人   \n",
       "2      爱玛科技集团股份有限公司   上市公司     10000人以上   \n",
       "3        天津卓漫科技有限公司     民营     500-999人   \n",
       "4      北京中阔鼎盛科技有限公司              20-99人   "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 以下为测试用代码\n",
    "get_data = transform_html(res)\n",
    "get_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 循环爬取每一页的数据，并且合成为一个dataframe\n",
    "def concat_data(url_lis):\n",
    "    # 定义字典储存dataframe\n",
    "    final_df_dict = {}\n",
    "    for url,num in zip(url_lis,range(len(url_lis))):\n",
    "        try:\n",
    "            print('开始爬取第{}页'.format(num+1))\n",
    "            # 获取网页源代码\n",
    "            response = get_html(url)\n",
    "            # 解析网页源代码并且生成一个dataframe\n",
    "            final_df = transform_html(response)\n",
    "            # 将dataframe保存到字典里\n",
    "            final_df_dict[num] = final_df\n",
    "            print('第{}页爬取完成'.format(num+1))\n",
    "            # 爬取完成后程序休眠10秒\n",
    "            time.sleep(10)\n",
    "        except:\n",
    "            print('所有页码都爬取完成！总计爬取{}页'.format(num+1))\n",
    "    concat_df = pd.concat(list(final_df_dict.values()),ignore_index = True)\n",
    "    return concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 爬取结果保存到csv\n",
    "def save_df(df,city):\n",
    "    file_name = '{}招聘信息.csv'.format(city)\n",
    "    path = r'E:\\2022年数据分析招聘数据爬取\\{}'.format(file_name)\n",
    "    df.to_csv(path,encoding = 'utf-8',index = False)\n",
    "    print('{}保存成功'.format(file_name))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用主函数，执行上边所有函数\n",
    "def main():\n",
    "    city,page = get_city_page()\n",
    "    url_lis = create_url_lis(city,page)\n",
    "    concat_df = concat_data(url_lis)\n",
    "    save_df(concat_df,city)\n",
    "    return print('所有程序执行完毕')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入要爬取的城市：郑州\n",
      "请输入要爬取的页码数：8\n",
      "开始爬取第1页\n",
      "第1页爬取完成\n",
      "开始爬取第2页\n",
      "第2页爬取完成\n",
      "开始爬取第3页\n",
      "第3页爬取完成\n",
      "开始爬取第4页\n",
      "第4页爬取完成\n",
      "开始爬取第5页\n",
      "第5页爬取完成\n",
      "开始爬取第6页\n",
      "第6页爬取完成\n",
      "开始爬取第7页\n",
      "第7页爬取完成\n",
      "开始爬取第8页\n",
      "第8页爬取完成\n",
      "郑州招聘信息.csv保存成功\n",
      "所有程序执行完毕\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
